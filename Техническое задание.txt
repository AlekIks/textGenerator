Генератор текстов

Задание: разработать утилиту, которая на основе заданных текстов генерирует свои.

Обязательная часть:
Считать входные данные. При считывании из файлов запрещается полностью загружать содержимое файлов в память. Единственное ограничение на размер файла - любая строчка файла заведомо помещается в память.
При считывании очищать тексты: выкидывать неалфавитные символы, опционально приводить к lowercase.
Разбить тексты на слова.
Подсчитать, насколько часто за одним словом следует другое. То есть для каждой пары <слово1>-<слово2> посчитать, сколько раз раз эта пара встречалась в тексте.
Сохранить получившуюся модель в файл таким образом, чтобы можно было восстановить слова и частоты.
Реализовать загрузку модели из файла.
Для заданного начального слова и заданной длины выводить сгенерированную последовательность слов на основе модели. Варианты реализации выбора:
На каждом шаге выбирается слово исходя из того, какое слово было предыдущим. Для <слово1> берутся частоты соответствующих <слов2>, на их основе составляется массив [<слово2-1>, повторённое <частота слова2-1> раз, <слово2-2>, повторённое <частота слова2-2-> раз, ...]. Выбор слова из этого массива производится с помощью random.choice.
Через кумулятивное распределение: https://stackoverflow.com/a/3679747.
Отнормировать частоты и сделать numpy.random.choice.
Написать комментарии и документацию к своему коду.

Сбор частот для модели и её использование (генерация текста) - 2 отдельных модуля, train.py и generate.py.

Ваша программа должна иметь консольный интерфейс, в котором реализовано как минимум следующее:
train.py:
--input-dir - путь к директории, в которой лежит коллекция документов. Если данный аргумент не задан, считать, что тексты вводятся из stdin.
--model - путь к файлу, в который сохраняется модель.
--lc - необязательный аргумент. Приводить тексты к lowercase.
--help - необязательный аргумент. Чтобы было понятно, как использовать ваш код.
generate.py:
--model - путь к файлу, из которого загружается модель.
--seed - необязательный аргумент. Начальное слово. Если не указано, выбираем слово случайно из всех слов (не учитывая частоты).
--length - длина генерируемой последовательности.
--output - необязательный аргумент. Файл, в который будет записан результат. Если аргумент отсутствует, выводить в stdout.
--help - необязательный аргумент. Чтобы было понятно, как использовать ваш код.

Особенности:
Для реализации консольного интерфейса удобно воспользоваться какой-нибудь библиотекой, например argparse
Для работы с текстами пригодится библиотека регулярных выражений (о том, что это такое можно почитать там же) re
Для проверки кодстайла пользуйтесь pep8. Корявые посылки проверяться не будут!

Базовые сопроводительные материалы:
https://en.wikipedia.org/wiki/N-gram
https://tproger.ru/translations/markov-chains/
https://habrahabr.ru/post/88514/

Бонусы:
Сложная токенизация (Как работать со словами с дефисом? Как работать с цифрами? С сокращениями?)
Обобщение с биграмм на n-граммы (делаем согласование между словами лучше)
Сбор текстов для обучения модели с какого-нибудь сайта
Ввод ограничений по частоте встречающихся слов для уменьшения размера модели
Сглаживание модели (Good–Turing, Katz's back-off model, Kneser–Ney smoothing, тык)
Включение морфологии (pymorphy2, делаем модель по грамматическим значениям, пересекаем с языковой)
Генерация с ограничениями (например, генерация пирожков)

Бонусные сопроводительные материалы:
https://lleo.me/soft/text_dip.htm
https://habrahabr.ru/post/334046/
https://arxiv.org/pdf/1602.02410.pdf
https://arxiv.org/abs/1312.3005
